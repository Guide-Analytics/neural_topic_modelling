{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591671b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef5cd1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model (trained on an enormous Google corpus)\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf26a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path,documents,reviewColumn):\n",
    "    # read review texts\n",
    "    reviews = []\n",
    "    for name in documents:\n",
    "        raw_data = pd.read_csv(path+name+\".csv\")\n",
    "        raw_data = raw_data.dropna(subset=[reviewColumn])\n",
    "        raw_data = raw_data.reset_index(drop=True)\n",
    "        reviews += list(raw_data[reviewColumn])\n",
    "    sentences = []\n",
    "    for review in reviews:\n",
    "        #split text into sentences (separated by .,?,!, or a newline)\n",
    "        sentences += re.split(r\"[\\.\\?!]+[ \\n]*\",review)\n",
    "    #split sentences into words\n",
    "    tokenized = [re.split(r\"[,]*[ \\n]+[,]*\",sentence) for sentence in sentences]\n",
    "    #remove empty sentences\n",
    "    tokenized = [element for element in tokenized if element!=['']]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea55ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word,vec_model):\n",
    "    try:\n",
    "        return vec_model[word]\n",
    "    except KeyError:\n",
    "        return np.zeros((300,1))\n",
    "\n",
    "def vectorize(sentence_text,vec_model):\n",
    "    return [tf.reshape(tf.convert_to_tensor(word2vec(word,vec_model),dtype=tf.float32),[300,1]) for word in sentence_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd5f7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence,M):\n",
    "    #calculate how much the model should focus on each word\n",
    "    a = embedding_focus(sentence,M)\n",
    "    #sentence embedding is a weighted average of the vectors in the sentence\n",
    "    z_s=tf.reduce_sum([a[i]*sentence[i] for i in range(len(sentence))],axis=0)\n",
    "    return z_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ad620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_focus(sentence,M):\n",
    "    y_s = tf.reshape(tf.reduce_mean(sentence,axis=0),[300,1])\n",
    "    a = tf.convert_to_tensor([tf.matmul(tf.matmul(tf.transpose(word),M),y_s) for word in sentence])\n",
    "    \n",
    "    #reduce values to prevent overflow\n",
    "    a -= tf.reduce_max(a)-70\n",
    "    #apply softmax\n",
    "    a = tf.exp(a)\n",
    "    a /= tf.reduce_sum(a)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a76601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_mean(sentence):\n",
    "    y_s = tf.reduce_mean(sentence,axis=0)\n",
    "    return y_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf81ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(embedding,W,b,T):\n",
    "    #calculate the topic proportion of the sentence based on the embedding and learned parameters\n",
    "    p_t = topic_proportion(embedding,W,b,T)\n",
    "    #get a weighted average of the topic vectors\n",
    "    r_s = tf.matmul(tf.transpose(T),p_t)\n",
    "    return r_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adc63945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_proportion(embedding,W,b,T):\n",
    "    #multiply by M and add bias b (both learned) to get prominance scores\n",
    "    p_t = tf.matmul(W,embedding)+b\n",
    "    #reduce values to prevent overflow\n",
    "    p_t -= tf.reduce_max(p_t)-70\n",
    "    #apply softmax\n",
    "    p_t = tf.exp(p_t)\n",
    "    p_t /= tf.reduce_sum(p_t)\n",
    "    return p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82b411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_s(embedding,re_embedding,negatives):\n",
    "    #calculate cosine similarity of reconstruction with embedding and a random sample of other embeddings\n",
    "    cos_sim = tf.reshape(tf.matmul(tf.transpose(embedding),re_embedding),[1])\n",
    "    neg_sim = tf.convert_to_tensor([tf.reshape(tf.matmul(tf.transpose(re_embedding),neg),[1]) for neg in negatives])\n",
    "    #loss tries to maximize cosine similarity of reconstruction with embedding \n",
    "    #while minimizing similarity with other (generally unrelated) sentence embeddings\n",
    "    return tf.reduce_sum(tf.maximum(0,1-cos_sim+neg_sim))\n",
    "\n",
    "def J(embeddings,reconstructions,negs):\n",
    "    total_loss = 0\n",
    "    dJ_dr_list = []\n",
    "    #get loss for each sentence and sum them\n",
    "    for i in range(len(embeddings)):\n",
    "        temp = J_s(embeddings[i],reconstructions[i],negs)\n",
    "        total_loss += temp\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "664377a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no longer used\n",
    "def regularization(T):\n",
    "    rows = []\n",
    "    #normalize topic vectors to length 1\n",
    "    for i in range(T.shape[0]):\n",
    "        rows.append([T[i]/tf.reduce_sum(tf.square(T[i]))])\n",
    "    normed = tf.concat(axis=0, values=rows)\n",
    "    #get dot product (=cosine similarity) of all pairs of topics\n",
    "    dots = tf.matmul(normed,tf.transpose(normed))\n",
    "    #subtract identity matrix (since the cosine similarity of all topics with itself is 1)\n",
    "    U = dots-tf.eye(T.shape[0])\n",
    "    #get norm of entire matrix\n",
    "    return tf.reduce_sum(tf.square(U))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8903499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topics(seed_lists,seed_weights,unseeded):\n",
    "    \n",
    "    #perform sparsemax of raw seed weights\n",
    "    weights_sorted = [tf.sort(element,direction='DESCENDING') for element in seed_weights]\n",
    "    k = [tf.range(element.shape[0],dtype='float32')+1 for element in seed_weights]\n",
    "    k_array = [1 + tf.math.multiply(k[i],weights_sorted[i]) for i in range(len(k))]\n",
    "    weights_cumsum = [tf.cumsum(element) for element in weights_sorted]\n",
    "    k_selected = [k_array[i] > weights_cumsum[i] for i in range(len(k_array))]\n",
    "    k_max = [tf.reduce_max(tf.where(element)).numpy()+1 for element in k_selected]\n",
    "    threshold = [(weights_cumsum[i][k_max[i]-1] - 1) / k_max[i] for i in range(len(weights_cumsum))]\n",
    "    seed_weights = [tf.maximum(seed_weights[i]-threshold[i],0) for i in range(len(seed_weights))]\n",
    "    \n",
    "    #seeded topoics are just weights averages of seed word vectors\n",
    "    seed_topics = tf.concat([tf.reshape(tf.matmul(tf.transpose(seed_lists[i]),seed_weights[i]),[1,300]) for i in range(len(seed_lists))],0)\n",
    "    all_topics = tf.concat([seed_topics,unseeded],0)\n",
    "    return all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "28e7c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = [\"dhl\",\"fedex\",\"ups\",\"usps\"]\n",
    "files = [\"sj_\"+str(i) for i in range(1,7)]\n",
    "files += [\"tp_\"+str(i) for i in range(1,6)]\n",
    "reviews = preprocess(\"input/shipping/\",files,'reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fde43379",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"reviews_google = []\n",
    "for i in range(2,6):\n",
    "    f = open(\"input/shipping_google_\"+str(i)+\".json\", encoding='utf-8')\n",
    "    raw = json.load(f)\n",
    "    for macro in raw:\n",
    "        for element in macro[\"reviews\"]:\n",
    "            if element[\"text\"]!=None:\n",
    "                reviews_google.append(element[\"text\"])\n",
    "sentences=[]\n",
    "for review in reviews_google:\n",
    "    sentences += re.split(r\"[\\.\\?!]+[ \\n]*\",review)\n",
    "tokenized = [re.split(r\"[,]*[ \\n]+[,]*\",sentence) for sentence in sentences]\n",
    "tokenized = [element for element in tokenized if element!=['']]\n",
    "reviews=tokenized\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a17ab88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27054/27054 [00:29<00:00, 905.95it/s] \n"
     ]
    }
   ],
   "source": [
    "vectorized_reviews = [vectorize(review,model) for review in tqdm(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2e452330",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMUNSEEDED = 1\n",
    "NUMSEEDED = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d86a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize parameters with small random values\n",
    "M = tf.random.uniform([300,300],minval=-1)\n",
    "b = tf.random.uniform([NUMSEEDED+NUMUNSEEDED,1],minval=-1)\n",
    "W = tf.random.uniform([NUMSEEDED+NUMUNSEEDED,300],minval=-1)\n",
    "#seed words\n",
    "seed_words = [[\"customs\",\"international\",\"country\"],\n",
    "             [\"service\",\"driver\"],\n",
    "             [\"paid\",\"worth\",\"fee\"],\n",
    "             [\"delivery\",\"package\",\"shipment\"],\n",
    "             [\"speed\",\"quick\",\"late\"],\n",
    "             [\"tracking\",\"email\",\"website\"]]\n",
    "seed_lists = [tf.concat([tf.reshape(model[word],[1,300]) for word in seed_words[i]],0) for i in range(len(seed_words))]\n",
    "seed_weights = [tf.random.uniform([len(seed_lists[i]),1]) for i in range(len(seed_lists))]\n",
    "seed_topics = tf.concat([tf.reshape(tf.matmul(tf.transpose(seed_lists[i]),seed_weights[i]),[1,300]) for i in range(len(seed_lists))],0)\n",
    "unseeded = tf.random.uniform([NUMUNSEEDED,300],minval=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0fed8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(vectorized_reviews,M,W,b,seed_lists,z,uT,m,negative_pool):\n",
    "    with tf.GradientTape(persistent=True) as g:\n",
    "        #track gradients\n",
    "        g.watch(M)\n",
    "        g.watch(W)\n",
    "        g.watch(b)\n",
    "        g.watch(z)\n",
    "        g.watch(uT)\n",
    "        T = build_topics(seed_lists,z,uT)\n",
    "        #get sentence embeddings\n",
    "        sentence_embeddings = [sentence_embedding(sentence,M) for sentence in vectorized_reviews]\n",
    "        #get sentence reconstructions\n",
    "        sentence_reconstructions = [reconstruction(embed,W,b,T) for embed in sentence_embeddings]\n",
    "        #choose random negative sentences\n",
    "        negs = random.sample(negative_pool,m)\n",
    "        #calculate loss over minibatch\n",
    "        total_loss = J(sentence_embeddings,sentence_reconstructions,negs)\n",
    "    grads = g.gradient(total_loss,{'M':M,'W':W,'b':b,'z':z,'uT':uT})\n",
    "    #return loss and gradients\n",
    "    return total_loss,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c53f0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(vectorized_reviews,M,W,b,seed_lists,z,uT,batch_size=50,learning_rate=0.001,m=20,beta_1=0.9,beta_2=0.999,epsilon=1e-8,num_epochs=1):\n",
    "    t=0\n",
    "    #initialize adam optimizer values to 0\n",
    "    m_M = tf.zeros(M.shape)\n",
    "    m_W = tf.zeros(W.shape)\n",
    "    m_b = tf.zeros(b.shape)\n",
    "    m_z = [tf.zeros(z[i].shape) for i in range(len(z))]\n",
    "    m_uT = tf.zeros(uT.shape)\n",
    "    v_M = tf.zeros(M.shape)\n",
    "    v_W = tf.zeros(W.shape)\n",
    "    v_b = tf.zeros(b.shape)\n",
    "    v_z = [tf.zeros(z[i].shape) for i in range(len(z))]\n",
    "    v_uT = tf.zeros(uT.shape)\n",
    "    #calculate the pool of negative sentences\n",
    "    #we use a straight mean rather than a full embedding\n",
    "    negative_pool = [sentence_mean(sentence) for sentence in tqdm(vectorized_reviews)]\n",
    "    for j in range(num_epochs):\n",
    "        #randomize order to avoid overfitting\n",
    "        random.shuffle(vectorized_reviews)\n",
    "        for i in tqdm(range(0,len(vectorized_reviews),batch_size)):\n",
    "            t+=1\n",
    "            \n",
    "            #get gradients through a forward pass with a minibatch\n",
    "            loss,grads=forward_pass(vectorized_reviews[i:min(len(vectorized_reviews),i+batch_size)],M,W,b,seed_lists,z,uT,m,negative_pool)\n",
    "            #update adam optimizer values\n",
    "            m_M = beta_1*m_M+(1-beta_1)*grads['M']\n",
    "            m_W = beta_1*m_W+(1-beta_1)*grads['W']\n",
    "            m_b = beta_1*m_b+(1-beta_1)*grads['b']\n",
    "            m_z = [beta_1*m_z[i]+(1-beta_1)*grads['z'][i] for i in range(len(z))]\n",
    "            m_uT = beta_1*m_uT+(1-beta_1)*grads['uT']\n",
    "\n",
    "            v_M = beta_2*v_M+(1-beta_2)*tf.square(grads['M'])\n",
    "            v_W = beta_2*v_W+(1-beta_2)*tf.square(grads['W'])\n",
    "            v_b = beta_2*v_b+(1-beta_2)*tf.square(grads['b'])\n",
    "            v_z = [beta_2*v_z[i]+(1-beta_2)*tf.square(grads['z'][i]) for i in range(len(z))]\n",
    "            v_uT = beta_2*v_uT+(1-beta_2)*tf.square(grads['uT'])\n",
    "            \n",
    "            #update parameters\n",
    "            M-=(m_M/(1-beta_1**t))/(tf.sqrt(v_M/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            W-=(m_W/(1-beta_1**t))/(tf.sqrt(v_W/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            b-=(m_b/(1-beta_1**t))/(tf.sqrt(v_b/(1-beta_2**t))+epsilon)*learning_rate\n",
    "            z=[z[i]-(m_z[i]/(1-beta_1**t))/(tf.sqrt(v_z[i]/(1-beta_2**t))+epsilon)*learning_rate for i in range(len(z))]\n",
    "            uT-=(m_uT/(1-beta_1**t))/(tf.sqrt(v_uT/(1-beta_2**t))+epsilon)*learning_rate\n",
    "    #return learned parameters\n",
    "    return M,W,b,z,uT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "02c418e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27054/27054 [00:02<00:00, 10699.62it/s]\n",
      "100%|██████████| 542/542 [08:12<00:00,  1.10it/s]\n",
      "100%|██████████| 542/542 [07:52<00:00,  1.15it/s]\n",
      "100%|██████████| 542/542 [07:48<00:00,  1.16it/s]\n",
      "100%|██████████| 542/542 [07:51<00:00,  1.15it/s]\n",
      "100%|██████████| 542/542 [07:56<00:00,  1.14it/s]\n",
      "100%|██████████| 542/542 [07:49<00:00,  1.15it/s]\n",
      "100%|██████████| 542/542 [07:50<00:00,  1.15it/s]\n",
      "100%|██████████| 542/542 [07:48<00:00,  1.16it/s]\n",
      "100%|██████████| 542/542 [07:49<00:00,  1.15it/s]\n",
      "100%|██████████| 542/542 [07:51<00:00,  1.15it/s]\n",
      "100%|██████████| 542/542 [07:46<00:00,  1.16it/s]\n",
      "100%|██████████| 542/542 [07:42<00:00,  1.17it/s]\n",
      "100%|██████████| 542/542 [07:43<00:00,  1.17it/s]\n",
      "100%|██████████| 542/542 [07:46<00:00,  1.16it/s]\n",
      "100%|██████████| 542/542 [07:44<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "M,W,b,seed_weights,unseeded = training_epoch(vectorized_reviews,M,W,b,seed_lists,seed_weights,unseeded,num_epochs=15)\n",
    "T = build_topics(seed_lists,seed_weights,unseeded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e4479ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With',\n",
       " 'fast',\n",
       " 'response',\n",
       " 'and',\n",
       " 'superior',\n",
       " 'professionism',\n",
       " 'I',\n",
       " 'think',\n",
       " 'Promoton',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'choice']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[7666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75d9a8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sentence = vectorized_reviews[7666]\n",
    "temp=sentence_embedding(temp_sentence,M)\n",
    "#temp=reconstruction(temp,W,b,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "892b81ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13, 1, 1), dtype=float32, numpy=\n",
       "array([[[1.0841736e-04]],\n",
       "\n",
       "       [[5.5329616e-05]],\n",
       "\n",
       "       [[3.4649510e-04]],\n",
       "\n",
       "       [[1.3073077e-06]],\n",
       "\n",
       "       [[9.3742259e-05]],\n",
       "\n",
       "       [[1.3073077e-06]],\n",
       "\n",
       "       [[9.4188362e-01]],\n",
       "\n",
       "       [[5.5953301e-02]],\n",
       "\n",
       "       [[1.3073077e-06]],\n",
       "\n",
       "       [[8.6980890e-07]],\n",
       "\n",
       "       [[7.1883419e-06]],\n",
       "\n",
       "       [[7.5633252e-05]],\n",
       "\n",
       "       [[1.4715591e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_focus(temp_sentence,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7208840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 1), dtype=float32, numpy=\n",
       "array([[9.6039370e-02],\n",
       "       [1.0486507e-03],\n",
       "       [8.9238565e-03],\n",
       "       [1.7518915e-02],\n",
       "       [2.9405186e-04],\n",
       "       [8.7617517e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_proportion(temp,W,b,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0e2cf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3537/3537 [00:11<00:00, 315.98it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_embeddings = [sentence_embedding(sentence,M) for sentence in tqdm(vectorized_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31d946f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3537/3537 [00:00<00:00, 6432.64it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_reconstructions = [topic_proportion(embed,W,b,T) for embed in tqdm(sentence_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7228bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUMSEEDED+NUMUNSEEDED):\n",
    "    file = codecs.open('output/shipping_topic_'+str(i)+\".txt\",'w','utf-8')\n",
    "    for j in range(len(reviews)):\n",
    "        if sentence_reconstructions[j][i]>=0.4:\n",
    "            file.write(str(j)+' '+ ' '.join(reviews[j]) +' '+str(sentence_reconstructions[j][i].numpy())+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61c370a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('customs', 0.9295828938484192), ('spokesman_Kees_Nanninga', 0.7020161747932434), ('Customs', 0.6367698311805725), ('customs_clearance', 0.5555130243301392), ('customs_formalities', 0.5413417220115662)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=[np.array(T[0])],topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab969fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,grads=forward_pass(vectorized_reviews,M,W,b,seed_lists,seed_weights,unseeded,20,[sentence_mean(sentence) for sentence in vectorized_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b26fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b361ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45857c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
